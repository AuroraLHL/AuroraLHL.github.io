<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Introduction to Machine Learning | Hongliang Lu </title> <meta name="author" content="Hongliang Lu"> <meta name="description" content="Graduate student at Peking University specializing in Agentic RL, WebAgent/OSAgent systems, and Self-Play training paradigms. Research focuses on enhancing LLM agent capabilities through novel RL approaches. Experienced RL algorithm engineer with internships at Alibaba and Moonshot AI. "> <meta name="keywords" content="agentic-rl, webagent, self-play, large-language-models, reinforcement-learning, optimization-algorithms, machine-learning, agents"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://auroralhl.github.io/blog/2024/ML1/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hongliang</span> Lu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to Machine Learning</h1> <p class="post-meta"> Created on July 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a>   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="一基本概念">一、基本概念</h2> <h3 id="什么是机器学习">什么是机器学习</h3> <p>总的来说，机器学习就是大数据时代背景下<strong>处理数据的各种方法</strong>的总称。机器学习利用各种数理模型对数据进行建模，对数据进行预测和分析，在实际应用中指导决策。</p> <p>我们通常将用于学习的数据对象或者实例称为<strong>样例或者样本</strong>，每个样例x采用一个向量 $x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T$来表示.</p> <ul> <li>向量的每个分量对应样例的一个<strong>特征或者属性.</strong> </li> <li>n为样例$x$的特征个数，也称为<strong>维数，</strong>$x^{(i)}$为样例x的第i 维属性的属性值.</li> <li>属性张成的空间$\chi$为<strong>特征空间，也称为样本空间或输入空间</strong>，记作$\chi$.</li> </ul> \[x=(x^{(1)},x^{(2)},\cdots,x^{(n)})^T\in\mathcal{X}\] <p>一般而言，数据对象的特征和学习任务相关，如果有不相关的特征可能会影响模型的预测能力。</p> <h3 id="机器学习的分类">机器学习的分类</h3> <p>机器学习一般包括监督学习、无监督学习、半监督学习、强化学习等。</p> <h4 id="监督学习">监督学习</h4> <p>监督学习 (Supervised learning)基于给定的<strong>标记数据集</strong>$T={(x_i,y_i)}_{i=1}^N$学习从输入空间$\chi$ 到输出空间$\gamma$ 的映射 (模型)，并利用该映射对未见 (unseen) 实例x对应的输出y进行预测。监督学习有两个核心问题：</p> <ul> <li> <p><strong>分类(classification)问题</strong>：输出空间$\gamma$是一个离散值的集合(通常也是有限的).</p> <ul> <li>$\mathcal{Y}={c_1,c_2,\cdots,c_M}$, 其中$M$为类别的个数.</li> <li> <p>二分类(binary classification)问题：$M=2.$</p> <ul> <li>$\mathcal{Y}={+1,-1}.$</li> <li>$\mathcal{Y}={0,1}.$</li> </ul> </li> <li>多分类(multi-class classification) 问题：$M&gt;2.$</li> </ul> </li> <li> <p><strong>回归 (regression) 问题</strong>：输出空间$\gamma=\mathbb{R}.$</p> </li> </ul> <h4 id="无监督学习">无监督学习</h4> <p>无监督学习( Unsupervised learning) 基于给定的<strong>无标记的数据集</strong>$T={x_i}_{i=1}^N$，发现数据中隐含的知识或者模式(interesting patterns)，并将学得的模式应用于未见实例。</p> <ul> <li>无监督学习通常也被称为知识发现(knowledge discovery)；</li> <li>通常没有明确的知识模式类型、衡量学习结果等的度量，依赖于具体学习场景和应用领域；</li> <li>更具有主观性和挑战性.</li> </ul> <p>一个典型的无监督学习任务——<strong>聚类</strong>，聚类的目的是将无标记的数据集$T={x_i}_{i=1}^N$划分成若干子集。</p> <ul> <li>这些子集通常互不相交，称每个子集为簇 N，每个簇对应于一个潜在的概念；</li> <li>属于同一子集的样本数据尽<strong>可能相互相似</strong>；</li> <li>不同子集的样本尽可能不同</li> </ul> <h4 id="半监督学习和强化学习">半监督学习和强化学习</h4> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202407241358100.png" alt="截屏2023-12-20 12.22.59"></p> <h2 id="二模型选择">二、模型选择</h2> <h3 id="机器学习三要素">机器学习三要素</h3> <p>机器学习方法都是由模型、策略和算法构成的，即机器学习方法由三要素构成，可以简单地表示为：</p> <blockquote> <p>机器学习方法＝<strong>模型＋策略＋算法</strong></p> </blockquote> <ul> <li> <strong>模型：</strong>在监督学习过程中，模型就是所要学习的<strong>条件概率分布或决策函数</strong>。模型的假设空间包含所有可能的条件概率分布或决策函数；</li> <li> <strong>策略：</strong>有了模型的假设空间，机器学习接着需要考虑的是按照什么样的<strong>准则</strong>学习或选择最优的模型；</li> <li> <strong>算法：</strong>算法是指学习模型的具体计算方法；</li> </ul> <h3 id="学习策略">学习策略</h3> <p>机器学习关心的问题是要学什么样的模型，在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型的<strong>假设空间(hypothesis space)</strong>包含所有可能的条件概率分布或决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合，假设空间中的模型一般有无穷多个。那么怎么选出最好的模型呢，首先引入损失函数与风险函数的概念：</p> <ul> <li> <strong>损失函数</strong>度量模型一次预测的好坏；</li> <li> <strong>风险函数</strong>度量平均意义下模型预测的好坏。</li> </ul> <p>在假设空间$\mathcal{F}$ 中选取模型 $f$ 作为决策函数，对于给定的输入 X,由 $f(X)$ 给出相应的输出$Y$,这个输出的预测值 $f(X)$ 与真实值$Y$ 可能一致也可能不一致，用一个<strong>损失函数(loss function)或代价函数(cost function)</strong>来度量预测错误的程度。损失函数是 $f(X)$ 和 Y 的非负实值函数，记作 $L(Y,f(X))$。机器学习常用的损失函数有以下几种：</p> <ol> <li> <p><strong>0-1 损失函数</strong>(0-1 loss function)</p> \[L(Y,f(X))=\begin{cases}&amp;1,\quad Y\neq f(X)\\&amp;0,\quad Y=f(X)\end{cases}\] </li> <li> <p><strong>平方损失函数</strong>(quadratic loss function)</p> \[L(Y,f(X))=(Y-f(X))^2\] </li> <li> <p><strong>对数损失函数</strong>(logarithmic loss function)或对数似然损失函数(log-likelihood loss function)</p> </li> </ol> \[\begin{aligned}L(Y,P(Y|X))&amp;=-\log P(Y|X)\end{aligned}\] <p>损失函数值越小，模型就越好。由于模型的输入、输出 $(X,Y)$ 是随机变量，遵循联合分布$P(X,Y)$,所以损失函数的期望是:</p> \[\begin{aligned} R_{\mathrm{exp}}(f)&amp; =E_{P}[L(Y,f(X))] \\ &amp;=\int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y. \end{aligned}\] <p>这是理论上模型 $f(X)$ 关于联合分布 $P(X,Y)$ 的平均意义下的损失，称为<strong>风险函数 (risk function) 或期望损失(expected loss)。</strong>由于联合分布$P(X,Y)$ 是不知道的，所以通常我们不会直接使用风险函数，而是用经验风险。给定一个训练数据集</p> \[T=\left\{\left(x_1, y_1\right),\left(x_2, y_2\right), \cdots,\left(x_N, y_N\right)\right\}\] <p>模型 $f(X)$ 关于训练数据集的平均损失称为<strong>经验风险 (empirical risk)</strong> , 记作 $R_{\text {emp }}$ :</p> \[R_{\text {emp }}(f)=\frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)\] <ul> <li>期望风险 $R_{\exp }(f)$ 是模型关于联合分布的期望损失；</li> <li>经验风险 $R_{\mathrm{emp}}(f)$ 是模型关于训练样本集的平均损失。</li> </ul> <p>根据大数定律, <strong>当样本容量 $N$ 趋于无穷时, 经验风险 $R_{\mathrm{emp}}(f)$ 趋于期望风险 $R_{\exp }(f)$</strong> 。所以一个很自然的想法是用经验风险估计期望风险。但是, 由于现实中训练样本数目有限, 甚至很小, 所以用经验风险估计期望风险常常并不理想, 要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略: 经验风险最小化和结构风险最小化。</p> <h4 id="经验风险最小化">经验风险最小化</h4> <p>在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数式(1.14) 就可以确定。经验风险最小化 (empirical risk minimization, ERM) 的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：</p> \[\min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i))\] <p>其中$\mathcal{F}$ 是假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。比如，极大似然估计(maximum likelihood estimation) 就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“<strong>过拟合” (over-fitting)</strong> 现象。</p> <h4 id="结构风险最小化">结构风险最小化</h4> <p><strong>结构风险最小化 (structural risk minimization, SRM) 是为了防止过拟合而提出来的策略</strong>。结构风险最小化等价于正则化(regularization)。结构风险在经验风险上加上表示模型复杂度的正则化项(regularizer)或罚项(penalty term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义是</p> \[R_{\mathrm{srm}}(f)=\frac1N\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)\] <ul> <li>其中，$J(f)$ 为模型的复杂度，是定义在假设空间$\mathcal{F}$上的泛函；</li> <li>模型 $f$ 越复杂，复杂度 $J(f)$ 就越大；反之，模型 $f$ 越简单，复杂度 $J(f)$ 就越小；</li> <li>$\lambda\geqslant0$ 是系数，用以权衡经验风险和模型复杂度；</li> <li>结构风险小需要经验风险与模型复杂度同时小，结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。</li> </ul> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202407241358063.png" alt="截屏2023-12-20 16.09.55" style="zoom:67%;"></p> <p>上图给出了模型复杂度与训练误差和测试误差的关系：</p> <ol> <li>模型复杂度小，训练误差较大，被称为<strong>欠拟合</strong>现象；</li> <li>模型复杂度过高，训练误差小，测试误差（泛化误差）大，被称为<strong>过拟合现象</strong>。</li> </ol> <p>比如，贝叶斯估计中的最大后验概率估计(maximum posterior probability estimation, MAP) 就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。</p> <p>结构风险最小化的策略认为结构风险最小的模型是最优的模型，所以求最优模型就是求解最优化问题：</p> \[\min_{f\in\mathcal{F}}\frac1N\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f).\] <h3 id="训练误差与泛化误差">训练误差与泛化误差</h3> <ul> <li>训练误差：模型在训练集上的<strong>经验风险</strong>，记为：$\hat{R}(f)=\frac1N\sum_{i=1}^NL(y_i,f(x_i))$；</li> <li>泛化误差：学习的模型在对未知数据的预测能力，也就是<strong>期望风险</strong>，记为$R(f)=E[L(Y,f(X))]$；</li> </ul> <p>通过经验风险最小化，选出的模型记为：</p> \[f_N = \arg \min_{f\in\mathcal{F}} \hat{R}(f).\] <p>$f_N$ 依赖训练数据集的样本容量 $N$。人们更关心的是 $f_N$ 的泛化能力</p> \[R(f_{N})=E[L(Y,f_{N}(X))].\] <p>下面讨论从有限集合$\mathcal{F}={f_1,f_2,\cdots,f_d}$ 中任意选出的函数 $f$ 的<strong>泛化误差上界</strong>。</p> <p><strong>定理 (泛化误差上界)</strong></p> <blockquote> <p>对二类分类问题，当假设空间是有限个函数的集合 $\mathcal{F}=$</p> <p>${f_1,f_2,\cdots,f_d}$ 时，对任意一个函数 $f\in\mathcal{F}$, 至少以概率 $1-\delta,0&lt;\delta&lt;1$, 以下不等式成立：</p> \[R(f)\leqslant\hat{R}(f)+\varepsilon(d,N,\delta)\] <p>其中：</p> \[\varepsilon(d,N,\delta) = \sqrt{\frac{1}{N}(\log d+\log\frac{1}{\delta})}.\] </blockquote> <p>从上述定理我们可以知道：</p> <ul> <li>$R(f)$ 是泛化误差，右端即为泛化误差上界；</li> <li>在泛化误差上界中，第1 项是训练误差，训练误差越小，泛化误差也越小；</li> <li>第2 项 $\varepsilon(d,N,\delta)$ <strong>是 N 的单调递减函数， 当$N$ 趋于无穷时趋于 0</strong>;</li> <li>同时它也是 $\sqrt{\log d}$ 阶的函数<strong>，假设空间 $\mathcal{F}$ 包含的函数越多，其值越大</strong>;</li> <li> <strong>训练误差$\hat{R}(f)$小并不能一定保证模型$f_N$的泛化性能好</strong>。</li> </ul> <p>泛化性能与学习算法捕获所有样本的共有知识模式的能力有关经验误差反映的是学习算法捕获训练数据蕴含的知识模式的能力。过小的训练误差可能导致所谓的过拟合 (Overfitting) 现象。</p> <h3 id="泛化误差的偏差-方差分解">泛化误差的偏差-方差分解</h3> <p>我们知道随着模型复杂度的增加，学习算法的学习能力越来越强，训练误差越来越小。而泛化误差先是随着训练误差的缩小而减小，但随着模型复杂度的进一步增加泛化误差不降反升。</p> <p>泛化误差与模型复杂度的关系为什么这样？ 针对回归任务，进一步讨论泛化误差由哪几个部分构成. 我们设$h_\mathrm{T}$是基于训练数据集$T$学习到的回归模型，对给定的$x$, 则学习算法的泛化误差为</p> \[E_T[(h_T(x)-c(x))^2].\] <p>其中，$c(x)$为真实标记。定义学习算法对数据$x$的<strong>期望输出</strong>为</p> \[\bar{h}(x)=E_T[h_T(x)].\] <p>$x$的期望输出与真实标记c$(x)$之间的差别称为<strong>偏差</strong>，即</p> \[Bias(x)=E_T[(h_T(x)-c(x)]=\bar{h}(x)-c(x).\] <ul> <li>偏差描述了学习算法对$x$的预测期望相对于x的真实输出的偏离程度；</li> <li> <strong>偏差反映了学习算法的学习能力，偏差越小，说明学习算法的学习能力越强</strong>.</li> </ul> <p>基于相同样本容量的不同训练数据集产生的<strong>预测方差</strong>为</p> \[Var(x)=E_T[(h_T(x)-\bar{h}(x))^2].\] <ul> <li>方差刻画学习算法使用相同容量的不同训练数据集所导致的学习性能的变动情况；</li> <li> <strong>方差越小，说明学习算法对数据扰动的容忍能力越强</strong>.</li> </ul> <p>接下来，我们对泛化误差进行如下分解：</p> \[\begin{aligned} E_T[(h_T(x)-c(x))^2] &amp;=E_T[h_T^2(x)-2h_T(x)c(x)+c^2(x)] \\ &amp;=E_T[h_T^2(x)]-2E_T[h_T(x)]c(x)+c^2(x) \\ &amp;= E_T[h_T^2(x)]-2\bar{h}(x)c(x)+c^2(x) \\ &amp;=E_T[h_T^2(x)]-\bar{h}^2(x)+\bar{h}^2(x)-2\bar{h}(x)c(x)+c^2(x) \\ &amp;=E_T[(h_T(x)-\bar{h}(x))^2]+(\bar{h}(x)-c(x))^2 \\ &amp;=-Var(x)+Bias^2(x). \\ \end{aligned}\] <p>这说明泛化误差可分解为<strong>方差和偏差</strong>的平方之和。由于噪声等的存在，使得$x$对应的观测$y$未必一定有</p> \[y=c(x).\] <p>我们不妨设</p> \[y=c(x)+\varepsilon,\] <p>其中$\varepsilon$为噪声，假定$\varepsilon$服从分布$\varepsilon$且其期望为0，即$E[\varepsilon]=0$.那么可以得到：</p> \[E_{T\sim D|T|,\varepsilon\sim\mathcal{E}}[(h_T(x)-y)^2]=Var(x)+Bias^2(x)+E[\varepsilon^2],\] <p>即泛化误差可以<strong>分解为方差、偏差和噪声三部分</strong>，其中噪声部分也称为不可约误差，反映了学习问题本身的难度.</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202407241358130.png" alt="截屏2023-12-20 16.36.21"></p> <p>方差和偏差通常是相互抵触的:</p> <ul> <li>当模型复杂度过于简单时，拟合能力比较弱，<strong>对数据扰动不敏感</strong>,此时偏差在泛化误差中起主导作用；</li> <li>随着模型复杂度的提高，算法的拟合能力不断增强，偏差逐渐减少。但学习能力的提高也带来过拟合的风险，使得学习算法<strong>对数据扰动逐渐敏感</strong>，方差在泛化误差中的比重逐渐增大，最终导致泛化误差不断增大。</li> </ul> <h3 id="正则化">正则化</h3> <p>正则化一般具有如下形式:</p> \[\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N L\left(y_i, f\left(x_i\right)\right)+\lambda \Omega(f)\] <p>其中:</p> <ol> <li>第 1 项是经验风险；</li> <li>第 2 项是正则化项, $\lambda \geqslant 0$ 为正则化参数；</li> </ol> <p>正则化项可以取不同的形式。例如, 在回归问题中, 正则化项可以是参数向量的 $L_2$ 范数:</p> \[L(w)=\frac{1}{N} \sum_{i=1}^N\left(f\left(x_i ; w\right)-y_i\right)^2+\frac{\lambda}{2}\|w\|^2\] <p>这里, $|w|$ 表示参数向量 $w$ 的 $L_2$ 范数。正则化项也可以是参数向量的 $L_1$ 范数:</p> \[L(w)=\frac{1}{N} \sum_{i=1}^N\left(f\left(x_i ; w\right)-y_i\right)^2+\lambda\|w\|_1\] <p>这里, $|w|_1$ 表示参数向量 $w$ 的 $L_1$ 范数。</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202309141039222.png" alt="截屏2023-09-14 10.39.28"></p> <h3 id="交叉验证">交叉验证</h3> <p>如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分成三部分，分别为训练集（training set）、验证集（validation set）和测试集（test set）。训练集用来训练模型，验证集用于模型的选择，而测试集用于最终对学习方法的评估。</p> <p>但是，在许多实际应用中数据是不充足的。为了选择好的模型，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复地进行训练、测试以及模型选择。</p> <p><strong>1 简单交叉验证(留出法）</strong></p> <blockquote> <p>简单交叉验证方法是：</p> <ol> <li>首先随机地将已给数据分为两部分，一部分作为训练集，另一部分作为测试集（例如，70％的数据为训练集，30％的数据为测试集）；</li> <li>然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型；</li> <li>在测试集上评价各个模型的测试误差，选出测试误差最小的模型。</li> </ol> </blockquote> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202312201744491.png" alt="截屏2023-12-20 17.43.46"></p> <p><strong>2 <em>K</em> 折交叉验证</strong></p> <blockquote> <p>应用最多的是 <em>K</em> 折交叉验证（<em>K</em>-fold cross validation），方法如下：</p> <ol> <li>首先将数据集$D$随机划分为$k$个互不相交、大小相似的子集$D_1,D_2,\cdots,D_k.$</li> <li>然进行$k$次训练-测试过程，其中第$i$次训练-学习过程中以$D-D_i$为训练数据集学得模型$h_{D-D_i}$</li> <li>以$D_i$为测试集对$h_{D-D_i}$进行测试评估，得到测试误差$\hat{R}<em>{test}(h</em>{D-D_i}).$</li> <li> <p>以</p> \[\frac1k\sum_{i=1}^k\hat{R}_{test}(h_{D-D_i})\] <p>作为$h_D$在本次数据集随机划分下的测试评估结果。</p> </li> <li>将这一过程对可能的 <em>K</em> 种选择重复进行；最后选出 <em>K</em> 次评测中平均测试误差最小的模型。</li> </ol> <p>比较耗时，每轮要训练K次。</p> </blockquote> <p><strong>3 留一交叉验证</strong></p> <blockquote> <p><em>K</em> 折交叉验证的特殊情形是 <em>K</em> = <em>N</em>，称为留一交叉验证（leave-one-out cross validation），往往在数据缺乏的情况下使用。这里，<em>N</em> 是给定数据集的容量。设数据集为$D$，其误差为：</p> \[\hat{R}(f_D)=\frac1{|D|}\sum_{x\in D}L(f_{D-\{x\}}(x),y).\] <ul> <li>也就是说每次测试集只有一个数据</li> <li>训练N次，数据量大时，计算开销大</li> </ul> </blockquote> <h3 id="自助法">自助法</h3> <p>自助法是为了解决交叉验证法在模型选择阶段<strong>训练集规模比整个样本小</strong>的问题，采用<strong>有放回抽样</strong>对交叉验证法进行改造。其具体策略如下：</p> <ol> <li> <table> <tbody> <tr> <td>先从 $D$ 中以有放回的抽样方式随机抽取 $</td> <td>D</td> <td>$ 个数据来构建训 练数据集 $T$,</td> </tr> </tbody> </table> </li> <li>然后以 $D$ 中没有被抽中的数据构建测试数据集 $T^{\prime}$.</li> </ol> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202309161228538.png" alt="截屏2023-09-16 12.28.29"></p> <ul> <li>自助法解决了交叉验证法中模型选择阶段和最终模型训练阶段的训练集规模差异问题.</li> </ul> <h2 id="三模型评估">三、模型评估</h2> <h3 id="回归评价指标">回归评价指标</h3> <h4 id="1-mse">1 MSE</h4> <p>回归问题最常用的评价指标是均方误差（Mean Square Error,MSE），其计算公式为：</p> \[MSE(y,\hat y)=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat y_i)^2\] <h4 id="2-rmse">2 RMSE</h4> <p>均方根误差（Root Mean Square Error,RMSE）：</p> \[RMSE(y,\hat y)=\sqrt{MSE(y,\hat y)}=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat y_i)^2}\] <p>均方误差和均方根误差通常会放大离群值对模型评估结果的影响，一种克服这个问题的方法是用，平均绝对误差（Mean Absolute Error,MAE)</p> <h4 id="3-mae">3 MAE</h4> <p>平均绝对误差（Mean Absolute Error,MAE):</p> \[MAE(y,\hat y)=\frac{1}{n}\sum_{i=1}^{n}|y_i-\hat y_i|\] <h3 id="分类评价指标">分类评价指标</h3> <p>在二分类问题中，每一个样本可以划分为以下四种类型：</p> <ol> <li>真正(True Positive , TP)：被模型预测为正的正样本。</li> <li>真负(True Negative , TN)：被模型预测为负的负样本。</li> <li>假正(False Positive , FP)：被模型预测为正的负样本。</li> <li>假负(False Negative , FN)：被模型预测为负的正样本。</li> </ol> <p>根据样本的真实标签和预测标签，可以得到一个混淆矩阵：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202312201756154.png" alt="截屏2022-05-18 下午9.11.58" style="zoom:67%;"></p> <p>正负样本选择，没有明确规定，根据应用场景进行选择，在机器学习中，往往将<strong>少数样本</strong>定义为正样本，表示希望<strong>模型在训练时更加关注少数样本</strong>。</p> <h4 id="1-正确率">1 正确率</h4> <p>分类问题中最常见的指标是正确率(accuracy)，表示<strong>模型预测正确的样本比例</strong>，给定混淆矩阵时，正确率计算公式如下：</p> \[正确率=\frac{TP+TN}{TP+TN+FP+FN}\] <p>在<strong>类别不均衡</strong>时，正确率不是一个很好的度量模型好坏的指标。</p> <blockquote> <p>比如，在文本情绪分类数据集中，有80%为正向的内容，20%为负向的内容，假如这个模型将所有样本都判断为正向，则正确率有80%!但是显然这个模型并不好</p> </blockquote> <p>对于类别不均衡的数据集，精度和召回率是比正确率更好的性能评价指标</p> <h4 id="2-精度">2 精度</h4> <p><strong>精度(precision)</strong>指正确预测的正样本占所有预测为正样本的比例：</p> \[精度=\frac{TP}{TP+FP}\] <h4 id="3-召回率">3 召回率</h4> <p>召回率（recall)又称为灵敏度或命中率，是指正样本中被正确预测的比例：</p> \[召回率=\frac{TP}{TP+FN}\] <p>通常精度和召回率是负相关的，实际应用中高精度往往对应低召回率，反之亦然，我们需要根据实际问题场景来选择哪一个指标。</p> <h4 id="4-f值">4 F值</h4> <p>单独考虑精度和召回率是片面的，需要综合考虑二者，在二分类问题中，可以定义一个综合考虑精度和召回率的指标，这个指标为F值：</p> \[F=\frac{(1+\beta^2)精度\times 召回率}{\beta^2\times精度+召回率}\] <p>其中$\beta$为正数，其作用是调节精度和召回率的权重，$\beta$越大，召回率的权重越大，$\beta$越小，精度的权重越大。当$\beta=1$时，它是精度和召回率的调和平均数：</p> <h2 id="参考资料">参考资料</h2> <ol> <li>李航 et al. 统计学习方法. 清华大学出版社, 2019.</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/nlp-fundamentals/">NLP Fundamentals: RNN, Seq2Seq and Attention Mechanism Basics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/scaling-law/">Scaling Law</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/attention-mechanism/">Understanding Attention Mechanism: Self-Attention and Attention Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/Githubpages_tutorial/">Building Personal Website with GitHub Pages</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hongliang Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>