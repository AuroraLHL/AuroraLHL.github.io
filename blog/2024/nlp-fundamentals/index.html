<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NLP Fundamentals: RNN, Seq2Seq and Attention Mechanism Basics | Hongliang Lu </title> <meta name="author" content="Hongliang Lu"> <meta name="description" content="Graduate student at Peking University specializing in Agentic RL, WebAgent/OSAgent systems, and Self-Play training paradigms. Research focuses on enhancing LLM agent capabilities through novel RL approaches. Experienced RL algorithm engineer with internships at Alibaba and Moonshot AI. "> <meta name="keywords" content="agentic-rl, webagent, self-play, large-language-models, reinforcement-learning, optimization-algorithms, machine-learning, agents"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://auroralhl.github.io/blog/2024/nlp-fundamentals/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hongliang</span> Lu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">NLP Fundamentals: RNN, Seq2Seq and Attention Mechanism Basics</h1> <p class="post-meta"> Created on July 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> LLM</a>   ·   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> AI</a>   <a href="/blog/category/large-language-models"> <i class="fa-solid fa-tag fa-sm"></i> Large Language Models</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>🔥 在自然语言处理（NLP）领域，理解和生成自然语言的能力对于构建智能系统至关重要。从文本分类、机器翻译到对话系统，底层技术的不断进步推动了NLP的发展。在这些技术中，循环神经网络（RNN）及其变种如长短期记忆网络（LSTM）、Seq2Seq模型和注意力机制（Attention Mechanism）扮演了重要角色。</p> <p>📚 本系列博客将从基础知识开始，逐步深入探讨大语言模型在NLP中的应用。我们将从RNN的经典结构出发，介绍其工作原理和局限性，接着探讨Seq2Seq模型及其在机器翻译中的应用，最后深入解读注意力机制及其在现代深度学习模型中的重要性。</p> <h2 id="一rnn经典结构">一、RNN经典结构</h2> <p><strong>循环神经网络（Recurrent Neural Network, RNN</strong>)是一类以序列数据为输入，在序列的演进方向进行递归且所有节点（循环单元）按链式连接的递归神经网络（recursive neural network）。对循环神经网络的研究始于二十世纪 80-90 年代，并在二十一世纪初发展为深度学习算法之一，其中双向循环神经网络和长短期记忆网络（Long Short-Term Memory networks，LSTM）是常见的循环神经网络。</p> <p>经典循环神经网络的单个神经元如下图所示，它由输入层、一个隐藏层和一个输出层组成：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405261918426.png" alt="截屏2024-05-26 19.18.11"></p> <ol> <li>$x_t$ 是一个向量，它表示<strong>t时刻</strong>输入层的值；</li> <li>$h_{t-1},h_t$ 分别表示<strong>t-1时刻和t时刻</strong>隐藏层的值.</li> </ol> <p>其中$h_t$的计算如下：</p> \[h_t=f(Uh_{t-1}+Wx_t+b),\] <p>其中$\mathbf{x}_t\in\mathbb{R}^M$ 为$t$时刻网络的输入，$\mathbf{U}\in\mathbb{R}^{D\times D}$ 为状态-状态权重矩阵，$\mathbf{W}\in\mathbb{R}^{D\times M}$为状态-输入权重矩阵，$\mathbf{b}\in\mathbb{R}^D$ 为偏置向量，$f$为<strong>激活函数</strong>.上面是RNN网络中的单个结点，下面给出长度为$T$的RNN网络，我们可以更清晰的看到前一时刻对后一时刻的影响.</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405261929403.png" alt="截屏2024-05-26 19.29.15"></p> <p>现在看上去就比较清楚了, 这个网络在 $\mathrm{t}$ 时刻接收到输入 $\mathrm{x}<em>{t}$ 之后, 隐藏层的值是 $h_t$, 输出值是 $\mathrm{y}</em>{t}$ 。关键一点是, $h_t$ 的值不仅仅取决于 $\mathrm{x}<em>{t}$, 还取决于 $\mathrm{h}</em>{t-1}$ 。可以用下面的公式来表示循环神经网络的计算方法:</p> \[\mathrm{y}_{t}=V \mathrm{h}_{t}\quad (1)\] \[\mathrm{h}_{t}=f\left(W \mathrm{x}_{t}+U \mathrm{h}_{t-1}\right)\quad(2)\] <ol> <li>式 1 是输出层的计算公式, 输出层是一个全连接层, 也就是它的每个节点都和隐藏层的每个节点相连，$\mathrm{V}$ 是输出层的权重矩阵.</li> <li>式 2 是隐藏层的计算公式, 它是循环层。 $\mathrm{U}$ 是输入 $x$ 的权重矩阵, $\mathrm{W}$ 是上一次的值 $\mathrm{h}_{t-1}$ 作为这一次的输入的权重矩阵, $f$ 是激活函数.（这里我们省略$b$）</li> </ol> <p>从上面的公式可以看出, 循环层和全连接层的区别就是循环层多了一个权重矩阵 $\mathrm{W}$ 。如果反复把式 2 带入到式 1 , 将得到:</p> \[\begin{aligned} \mathrm{y}_{t} &amp;=V \mathrm{h}_{t} \\ &amp;=V f\left(W \mathrm{x}_{t}+U \mathrm{h}_{t-1}\right) \\ &amp;=V f\left(W \mathrm{x}_{t}+U f\left(W \mathrm{x}_{t-1}+U \mathrm{h}_{t-2}\right) \right)\\ &amp;=V f\left(W \mathrm{x}_{t}+U f\left(W \mathrm{x}_{t-1}+U f\left(W \mathrm{x}_{t-2}+U \mathrm{h}_{t-3}\right)\right)\right) \\ &amp;=V f\left(W \mathrm{x}_{t}+U f\left(W \mathrm{x}_{t-1}+U f\left(W \mathrm{x}_{t-2}+U f\left(W \mathrm{x}_{t-3}+\ldots\right)\right)\right)\right) \end{aligned}\] <p>从上面可以看出, 循环神经网络的输出值$\mathrm{y}_t$, 是受前面历次输入值 $ x<em>{t}, x</em>{t-1}, \cdots,x_1$影响的, 这就是为什么循环神经网络可以往前看任意多个输入值的原因。</p> <p><strong>Note:</strong></p> <ul> <li>从以上结构可看出，<strong>传统的RNN结构的输⼊和输出等⻓.</strong> </li> <li>我们可以看到每个节点会<strong>共用参数</strong>$W,U,V$.</li> </ul> <h2 id="二vector-to-sequence结构">二、vector-to-sequence结构</h2> <p>有时我们要处理的问题输⼊是⼀个单独的值，输出是⼀个序列。</p> <p>此时，有两种主要建模⽅式：</p> <ol> <li> <p><strong>⽅式⼀：</strong>可只在其中的某⼀个序列进⾏计算，⽐如序列第⼀个进⾏输⼊计算，其建模⽅式如下：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281636083.png" alt="截屏2022-06-02 下午8.13.43"></p> </li> <li> <p>把输⼊信息X作为每个阶段的输⼊，其建模⽅式如下：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281636216.png" alt="截屏2022-06-02 下午8.14.19"></p> </li> </ol> <h2 id="三sequence-to-vector结构">三、sequence-to-vector结构</h2> <p>有时我们要处理的问题输⼊是⼀个序列，输出是⼀个单独的值，此时通常在最后的⼀个序列上进⾏输出变换，其建模如下所⽰：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281636872.png" alt="截屏2024-05-26 16.56.09"></p> <h2 id="四seq2seq模型">四、Seq2Seq模型</h2> <p>RNN最重要的一个变种：Seq2Seq，这种结构又叫Encoder-Decoder模型。</p> <p>原始的sequence-to-sequence结构的RNN要求序列等长，然⽽我们遇到的⼤部分问题序列都是不等长的，如机器翻译中，源语⾔和⽬标语⾔的句⼦往往并没有相同的长度。</p> <h3 id="1-encoder">1 Encoder</h3> <p>将输⼊数据编码成⼀个上下⽂向量 c，这部分称为<strong>Encoder</strong>，其⽰意如下所⽰：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281637454.png" alt="截屏2022-06-02 下午8.19.38"></p> <p>得到c有多种⽅式：</p> <ol> <li>最简单的⽅法就是把Encoder的最后⼀个隐状态赋值给c ：$c=h_4$</li> <li>还可以对最后的隐状态做⼀个变换得到 :$c=q(h_4)$</li> <li>也可以对所有的隐状态做变换:$c=q(h_1+h_2+h_3+h_4)$</li> </ol> <h3 id="2-decoder">2 Decoder</h3> <p><strong>拿到c之后，就用另一个RNN网络对其进行解码</strong>，这部分RNN网络被称为<strong>Decoder</strong>。具体做法就是将c当做初始状态输入到Decoder中：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281637667.png" alt="截屏2022-06-02 下午8.24.57"></p> <p>还可以将c作为Decoder的每⼀步输⼊，⽰意图如下所⽰：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202405281637809.png" alt="截屏2022-06-02 下午8.26.20"></p> <p>由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p> <ul> <li>机器翻译，Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的；</li> <li>文本摘要，输入是一段文本序列，输出是这段文本序列的摘要序列；</li> <li>阅读理解，将输入的文章和问题分别编码，再对其进行解码得到问题的答案；</li> <li>语音识别，输入是语音信号序列，输出是文字序列。</li> </ul> <h2 id="五embedding">五、Embedding</h2> <h3 id="1-什么是-embedding">1 什么是 Embedding？</h3> <p>Embedding 是一种将离散的、高维的输入数据（如单词或符号）转换为连续的、低维的向量表示的方法。其目的是捕捉输入数据中的语义和结构信息，使其在向量空间中具有有意义的分布。</p> <h3 id="2-为什么需要-embedding">2 为什么需要 Embedding？</h3> <p>在 NLP 中，原始的文本数据通常由单词、子词或字符组成，这些元素本质上是离散的符号。传统的处理方法（如 one-hot 编码）会将每个单词表示为一个高维的稀疏向量，缺点如下：</p> <ol> <li> <strong>维度过高</strong>：词汇表中的每个单词都需要一个独立的维度，对于大型词汇表，这会导致向量的维度非常高。</li> <li> <strong>稀疏性</strong>：大多数维度都是 0，只有一个维度是 1，这导致向量非常稀疏。</li> <li> <strong>缺乏语义信息</strong>：one-hot 向量无法捕捉单词之间的语义关系，如“国王”与“王后”之间的关系。</li> </ol> <p>Embedding 方法通过学习一个低维的、连续的向量表示来解决这些问题。每个单词或符号都被表示为一个实数向量，这些向量在训练过程中被优化，以捕捉单词之间的语义和语法关系。</p> <h3 id="3-embedding-的实现">3 Embedding 的实现</h3> <p>在深度学习模型（如 Transformer）中，embedding 通常通过一个可训练的查找表实现。查找表的每一行对应一个词汇表中的单词，行中的值是该单词的嵌入向量。这些嵌入向量在模型训练过程中不断调整，以优化模型的性能。</p> <p>具体步骤如下：</p> <ol> <li> <strong>初始化</strong>：将每个单词初始化为一个随机的或预训练的向量。</li> <li> <strong>训练</strong>：在训练过程中，模型根据任务目标不断调整这些向量，使得它们能够更好地表示单词的语义和语法信息。</li> <li> <strong>使用</strong>：训练完成后，这些向量就可以用于各种 NLP 任务，如文本分类、情感分析、机器翻译等。</li> </ol> <h3 id="4-例子">4 例子</h3> <p>假设我们有一个简单的词汇表：[“猫”, “狗”, “鱼”]。通过 embedding 层，这些单词会被转换为低维的向量，如：</p> <ul> <li>“猫” -&gt; [0.2, -1.3, 0.5]</li> <li>“狗” -&gt; [0.3, -1.2, 0.4]</li> <li>“鱼” -&gt; [-0.5, 0.8, -0.1]</li> </ul> <p>这些向量的维度通常比原始 one-hot 向量的维度小得多（如 300 维，而不是数万维），并且这些向量可以捕捉到单词之间的语义相似性。例如，“猫”与“狗”的向量可能比较接近，而“鱼”的向量则相对远一些。</p> <h3 id="5-总结">5 总结</h3> <p>Embedding 是一种将离散的输入数据转换为连续的、低维向量表示的方法，广泛应用于自然语言处理任务中。它能够有效地捕捉单词之间的语义和语法关系，提高模型的处理能力和表现。在 Transformer 等深度学习模型中，embedding 层是关键组成部分，负责将输入序列转换为模型可以处理的向量形式。</p> <h2 id="六实例机器翻译">六、实例：机器翻译</h2> <p>神经机器翻译（Neural Machine Translation, NMT）是一种基于深度学习技术的机器翻译方法。与传统的统计机器翻译（Statistical Machine Translation, SMT）不同，NMT 使用神经网络模型来<strong>直接建模源语言到目标语言之间的翻译过程。</strong>下图是Encoder编码的过程：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202407091030337.png" alt="截屏2024-07-09 10.30.50"></p> <p>Note:</p> <ol> <li>在翻译的时候，我们需要对词进行Embedding，将其变成一个连续的向量表达.</li> </ol> <p>Decoder的过程如下图所示：</p> <p><img src="https://aurora-pics.oss-cn-beijing.aliyuncs.com/Pic/202407091033500.png" alt="截屏2024-07-09 10.33.31"></p> <p>Note:</p> <ol> <li>用Softmax来得到预测的每个词的概率，概率最大的作为当前预测词</li> </ol> <h2 id="参考资料">参考资料</h2> <ol> <li><a href="https://github.com/scutan90/DeepLearning-500-questions" rel="external nofollow noopener" target="_blank">深度学习500问</a></li> <li>邱锡鹏，神经网络与深度学习，机械工业出版社，<a href="https://nndl.github.io/,%202020." rel="external nofollow noopener" target="_blank">https://nndl.github.io/, 2020.</a> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/attention-mechanism/">Understanding Attention Mechanism: Self-Attention and Attention Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/transformer/">Transformer Architecture Explained: Attention is All You Need</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/ML1/">Introduction to Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/scaling-law/">Scaling Law</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hongliang Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>